{\rtf1\ansi\ansicpg1252\cocoartf1138
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 # High-performance Computing Platforms\
\
----------\
\
## Homework Review\
\
### Performance\
\
1\
    a. 714ns\
    b. 3000ns / 714ns = 4.2\
\
2\
    ((1 cycle) * 0.95) + (((2 cycles) * 0.8 + (12 cycles) * 0.2) * 0.05) = 1.15 cycles\
\
### Architectures\
\
1\
    P6 can put the information in all the memory banks, so the other processes can access from there.\
\
2\
\
log2(p)\
\
3\
\
    a. With wraparound := 2d\
       Without wraparound := d\
\
    b. With wraparound := 2 * |_ sqrt(p) / 2 _|\
       Without wraparound := d(^d sqrt(p) - 1)\
\
    c. Each node reaches out around itself in each dimension.\
\
----------\
\
## Lesson\
\
### Design Process of Parallel Algorithms\
\
- Identify potentially concurrent work\
- Map concurrent work onto parallel tasks\
- Specify/distribute data\
- Synchronize data access\
- Synchronize process execution\
\
### Background\
\
- Task: Programmer-defined unit of computation\
- Decomposition: Dividing work into smaller components\
- Granularity: "Size" of task\
- Degree of concurrency:  Max number of tasks\
  - Task-dependency graph: Representation of inherent data dependencies\
  - Task-interaction graph: Communication\
  - Critical path: Limiting dependencies\
\
Task-dependency graph example:\
\
    a = b + c\
    x = y + z\
    m = a * x\
    t = 0.2 * m\
\
    a = b + c  \
               \\\
                 >>  m = a * x  >>  t = 0.2 * t\
               /\
    x = y + z\
\
#### Task Characteristics\
\
- Uniform/non-uniform size\
- Static/dynamic generated\
- Static/dynamic communication\
- Mapping: Assigning tasks to execution units\
\
#### Decomposition\
\
- Recursive: Divide & Conquer <=> e.g. merge sort\
- Data: Data-parallel programming (SIMD)\
- Exploratory: Search space <=> e.g. 8-queens problem\
- Speculative: cost-heuristic evaluation <=> e.g. markov-chain\
\
#### Load Balancing (mapping)\
\
- **Static**: Data partitioning\
  - Block distribution\
  - Cyclic: Interleaved\
  - Randomized\
  - Mesh: Irregular\
  - Hierarchical: Tree-based\
- **Dynamic**\
  - List scheduling\
\
### Models\
\
- Data-parallel (SIMD): Each task performs the same operation on different data\
- Task graph: Implementation of task/data dependency graph\
- Worker pool: "Bag of tasks", list scheduling\
- Master-worker: Master generates/assigns tasks to workers\
  - Different from bag in the fact that here different workers are better for specific tasks\
- Pipeline (Stream-parallel): Producer-consumer relationship\
}